{
  "cells": [
    {
      "metadata": {
        "_uuid": "53c90f720c67f85516c2e9350017d9da94185435"
      },
      "cell_type": "markdown",
      "source": "# Exploring Normalizing Data \n## By Jeff Hale"
    },
    {
      "metadata": {
        "_uuid": "1a3ebdc6322fff5995c5801693a596fd262b3105"
      },
      "cell_type": "markdown",
      "source": "I've been working on figuring out when to use sklearn's MinMaxScaler, StandardScaler, and Normalizer for preprocessing data for machine learning. As often as these topics show up in machine learning workflows, I had a tough time finding good explanations of the purposes of the different methods.\n\nThe nomenclature gets confused quite a bit. Writers often use the terms *scale* and *normalize* interchangeably. Here's my attempt to clarify. Please upvote if you find it helpful.\n\nAs part of this project I found Ben Alex Keen's helpful [post ](http://benalexkeen.com/feature-scaling-with-scikit-learn/) on the topic and used his code snippets liberally as a starting point below.\n\nThis post was originally created on [Kaggle.](https://www.kaggle.com/discdiver/scale-standardize-or-normalize-with-sklearn/edit)"
    },
    {
      "metadata": {
        "_uuid": "8174f53e7350d7f282636b7d5a94d8299c554552"
      },
      "cell_type": "markdown",
      "source": "## Why scale, standardize, or normalize?\n\nSome machine learning algorithms, such as neural networks, regression-based algorithms, K-nearest neighbors algorithms, support vector machines with radial bias kernel functions, principal components analysis, and algorithms using linear discriminant analysis don't perform as well if the feature data is not on a relatively similar scale. Some times you'll find you want a more normally distributed distribution. Some of the methods below dilute the effects of outliers. \n\nWe're gong to to stick to scaling, standardizing, and normalizing in this project. We aren't looking at transforming data with the log transformation or other transformations aimed at reducing the homoscedacity of errors that you might be shooting for in a regression model. We leave that for a future project."
    },
    {
      "metadata": {
        "_uuid": "879ed31012436b94008171dedfeac9344092432d"
      },
      "cell_type": "markdown",
      "source": "## TLDR: \n\n* Use MinMaxScaler as the default\n* Use RobustScaler if you have outliers\n* Use StandardScaler if you have relatively normal distributions and just need to standardize them\n* Use Normalizer sparingly"
    },
    {
      "metadata": {
        "_uuid": "d9dcb9829af0203c65deaaa72da6f03b26246f67"
      },
      "cell_type": "markdown",
      "source": "Here's a [cheat sheet I made in a google sheet](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) to help folks keep the options straight. ![](https://www.dropbox.com/s/qdkzdy94fbduqnn/Scale%2C%20standardize%2C%20or%20normalize%20chart%20Jeff%20Hale.png?dl=0)\n\nI'm looking for a good way to quickly plug a formated google sheet into a Jupyter notebook. If you know of a way, please share in the comments."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "57351b9e010c0261783bd5bbbde7db795df33bcb"
      },
      "cell_type": "markdown",
      "source": "## MinMaxScaler\n\nPreserves the shape of the original distribution. Doesn't reduce the importance of outliers. Least disruptive to the information in the original data. The default range for the feature returned by MinMaxScaler is 0 to 1 but that can be overridden. \n\nScaling a feature means to add or substract a constant and then multiply or divide by another constant. MinMaxScaler subtracts the mimimum value in the feature and then divide by the difference between the original maximum and original minimum.\n\nIt's a good place to start unless you know you want a normal distribution or have outliers you want to preserve."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8c575107edeb898153e08a51820645de7d58cb2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# based on http://benalexkeen.com/feature-scaling-with-scikit-learn/\nnp.random.seed(34)\ndf = pd.DataFrame({\n    # positive skew\n    'x1': np.random.chisquare(8, 1000),\n    # negative skew \n    'x2': np.random.beta(8, 2, 1000) * -40,\n    # no skew\n    'x3': np.random.normal(50, 3, 1000) * 10,\n    # negative normal\n    'x4': np.random.normal(50, 3, 1000) * -10\n})\n\nscaler = preprocessing.MinMaxScaler()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3', 'x4'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\nsns.kdeplot(df['x4'], ax=ax1)\n\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(scaled_df['x1'], ax=ax2)\nsns.kdeplot(scaled_df['x2'], ax=ax2)\nsns.kdeplot(scaled_df['x3'], ax=ax2)\nsns.kdeplot(scaled_df['x4'], ax=ax2)\n\nplt.show()\nprint(\"After MinMax Scaling\")\nprint(\"x1 mean = {}\".format(scaled_df.x1.mean()))\nprint(\"x1 min = {}\".format(scaled_df.x1.min()))\nprint(\"x1 max = {}\".format(scaled_df.x1.max()))\nprint(\"x3 mean = {}\".format(scaled_df.x3.mean()))\nprint(\"x3 max = {}\".format(scaled_df.x3.max()))\nprint(\"x4 mean = {}\".format(scaled_df.x4.mean()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4736e8f80abb81546c373a5a24e64e2abbbff7ab"
      },
      "cell_type": "markdown",
      "source": "## RobustScaler"
    },
    {
      "metadata": {
        "_uuid": "4a0f70e0a348b21c264c06e1dce71774c73b41c2"
      },
      "cell_type": "markdown",
      "source": "The RobustScaler uses a similar method to the MinMaxScaler but divides by the interquartile range instead of the the difference between the original maximum and original minimum. Use this if you have outliers and want to preserve them. The returned feature's range will vary more widely than MinMax Scaler's."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90a2da3645fce82e8557f94eeb615461d7062411",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 1, 1000)*-5, np.random.normal(1, 1, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n})\n\nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(x)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(x)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['x1', 'x2'])\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(9, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(x['x1'], ax=ax1)\nsns.kdeplot(x['x2'], ax=ax1)\n\nax2.set_title('After Robust Scaling')\nsns.kdeplot(robust_scaled_df['x1'], ax=ax2)\nsns.kdeplot(robust_scaled_df['x2'], ax=ax2)\n\nax3.set_title('After Min-Max Scaling')\nsns.kdeplot(minmax_scaled_df['x1'], ax=ax3)\nsns.kdeplot(minmax_scaled_df['x2'], ax=ax3)\nplt.show()\n\nprint(\"Means after robust scaling:\")\nprint(\"x1 mean: {}\".format(robust_scaled_df.x1.mean()))\nprint(\"x2 mean: {}\".format(robust_scaled_df.x2.mean()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2c0c9d797b2e4383ed884434cc85eb09ef4c508"
      },
      "cell_type": "markdown",
      "source": "Let's test RobustScaler's performance with some variations in the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78acacaab8d1f323bc3124e8d21c1ee1d6662b59",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(-30, 1, 1000), np.random.normal(50, 1, 25)]),\n})\n\nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(x)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(x)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['x1', 'x2'])\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(9, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(x['x1'], ax=ax1)\nsns.kdeplot(x['x2'], ax=ax1)\nax2.set_title('After Robust Scaling')\nsns.kdeplot(robust_scaled_df['x1'], ax=ax2)\nsns.kdeplot(robust_scaled_df['x2'], ax=ax2)\nax3.set_title('After Min-Max Scaling')\nsns.kdeplot(minmax_scaled_df['x1'], ax=ax3)\nsns.kdeplot(minmax_scaled_df['x2'], ax=ax3)\nplt.show()\n\nprint(\"Means after robust scaling:\")\nprint(\"x1 mean: {}\".format(robust_scaled_df.x1.mean()))\nprint(\"x2 mean: {}\".format(robust_scaled_df.x2.mean()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c4b6f10ccd1249107e39f37384561ed0b6e00db"
      },
      "cell_type": "markdown",
      "source": "Notice how RobustScaler brings the distributions together in the presence of outliers much more so than MinMaxScaler."
    },
    {
      "metadata": {
        "_uuid": "e44757c575e2e895ea7e9ce389311de305db7725"
      },
      "cell_type": "markdown",
      "source": "## StandardScaler"
    },
    {
      "metadata": {
        "_uuid": "c30550bf33c688b4301d39bae42133993472ceb8"
      },
      "cell_type": "markdown",
      "source": "StandardScaler is used to standardize features by removing the mean and scaling to unit variance. Unit variance means you divide all the values by the standard deviation results in a distribution with a standard deviation equal to 1 (variance equal to 1 squared = 1). You'll want your data to be normally distributed to use StandardScaler. After using on a feature with enough data points the mean will be zero, and about 68% of the data should be between -1 and 1. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c14c65f0c9f441201404e5e74d23375220934c1a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "np.random.seed(1)\ndf = pd.DataFrame({\n    'x1': np.random.normal(0, 2, 10000),\n    'x2': np.random.normal(5, 3, 10000),\n    'x3': np.random.normal(-5, 5, 10000)\n})\n\nscaler = preprocessing.StandardScaler()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\nax2.set_title('After Standard Scaler')\nsns.kdeplot(scaled_df['x1'], ax=ax2)\nsns.kdeplot(scaled_df['x2'], ax=ax2)\nsns.kdeplot(scaled_df['x3'], ax=ax2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9b5cff08e8074d6c35a77c45d10011778b6018e"
      },
      "cell_type": "markdown",
      "source": "Look at that overlap after StandardScaler has done it's business. x1 is just a tad more leptokurtic than x2 and x3. "
    },
    {
      "metadata": {
        "_uuid": "60e1c42a6dbccdabbe1db53db90c1861edc96ebb"
      },
      "cell_type": "markdown",
      "source": "## Normalizer"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4b74edb9cc24adb20a3ebc9c32bf9d6ab74c460f"
      },
      "cell_type": "markdown",
      "source": "Normalizer works on the rows, not the columns! I went through almost this entire exercise before I caught that.\n\nData is often normalized by applying L2 normalization to scale the element to have a unit norm. “Unit norm” with L2 means that if each element were squared and summed, the total would equal 1. \n\nJeremy's post at [http://kawahara.ca/how-to-normalize-vectors-to-unit-norm-in-python/](http://kawahara.ca/how-to-normalize-vectors-to-unit-norm-in-python/) helped me see how Normalizer works..\n\nIn most cases one of the other preprocessing tools above will be more helpful."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56781d4ddfd0b5039f52ba717bc1214e155e9d89",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "np.random.seed(1)\ndf = pd.DataFrame({\n    'x1': np.random.normal(-90, 2, 10000),\n    'x2': np.random.normal(10, 3, 10000),\n    'x3': np.random.normal(-5, 5, 10000)\n})\n\nnormalizer = preprocessing.Normalizer()\nnormed_df = normalizer.fit_transform(df)\nnormed_df = pd.DataFrame(normed_df, columns=['x1', 'x2', 'x3'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Normalizer')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\n\nax2.set_title('After Normalizer')\nsns.kdeplot(normed_df['x1'], ax=ax2)\nsns.kdeplot(normed_df['x2'], ax=ax2)\nsns.kdeplot(normed_df['x3'], ax=ax2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3805889e6b84191890eb5ab3ddfc8252eaebe24e"
      },
      "cell_type": "markdown",
      "source": "Another example exploring what Normalizer does to data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d32c4de23eb954a8b8810d29b038c182611bd9d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": " df = pd.DataFrame({\n    'z1': [ 1., -1.,  2., 555],\n    'z2': [ 2.,  0.,  0., -10],\n    'z3': [ 0.,  1., -1., 0],\n    'z4': [ 3, 5, 7, 9]\n})\n\nnormalizer = preprocessing.Normalizer()\nnormed_df = normalizer.fit_transform(df)\nnormed_df = pd.DataFrame(normed_df, columns=['z1', 'z2', 'z3', 'z4'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Normalizer')\nsns.kdeplot(df['z1'], ax=ax1)\nsns.kdeplot(df['z2'], ax=ax1)\nsns.kdeplot(df['z3'], ax=ax1)\nsns.kdeplot(df['z4'], ax=ax1)\n\nax2.set_title('After Normalizer')\nsns.kdeplot(normed_df['z1'], ax=ax2)\nsns.kdeplot(normed_df['z2'], ax=ax2)\nsns.kdeplot(normed_df['z3'], ax=ax2)\nsns.kdeplot(normed_df['z4'], ax=ax2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e4d89e91c12463c0060ce7cd9c0181148629b65"
      },
      "cell_type": "markdown",
      "source": "Normalizer does bring all the features into closer scale with each other. Could be helpful for some sparse datasets. Seems like a rather blunt instrument to me. Have you found good use cases for Normalizer? If so, please share in the comments.\n\nSpecial thanks again to Ben Alex Keen."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}